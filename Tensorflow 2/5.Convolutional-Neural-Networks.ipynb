{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72786e16-4fa5-45ba-9110-1c5599086fbd",
   "metadata": {},
   "source": [
    "# **Convolutional Neural Networks (CNNs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b14cd5-2f3d-4b39-a4df-596fa603d23d",
   "metadata": {},
   "source": [
    "In this section, we are going to discuss about yet powerful, famous model - Convolutional Neural Networks (CNNs). <br>\n",
    "CNNs are primarily used for image recognition and processing, due to its ability to recognize patterns in the images. <br>\n",
    "It is a powerful tool but requires a lot of (usually in millions) labelled training data points for effective training of the model. <br>\n",
    "Here, let's train a CNN for CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7b4f6df-6eeb-4c05-9e67-e7af9bd4a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers \n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7e2535-acc2-4bd3-9b6c-78ff527d5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "(X_train, y_train),(X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4080a7a-2df5-440b-aed7-451db373699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing: Converting 64bits to 32bits for computational efficiency \n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Note that here the array wasn't flattened - CNNs take input in it's original shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa4755f7-1a0d-460a-b5e9-c25b2062f40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) uint8\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 22s 26ms/step - loss: 1.6769 - accuracy: 0.3911\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 23s 30ms/step - loss: 1.3609 - accuracy: 0.5173\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 23s 30ms/step - loss: 1.2261 - accuracy: 0.5686\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 24s 30ms/step - loss: 1.1304 - accuracy: 0.6062\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 24s 30ms/step - loss: 1.0659 - accuracy: 0.6297\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 25s 31ms/step - loss: 1.0096 - accuracy: 0.6525\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 24s 30ms/step - loss: 0.9605 - accuracy: 0.6693\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 24s 31ms/step - loss: 0.9216 - accuracy: 0.6825\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.8861 - accuracy: 0.6944\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 24s 31ms/step - loss: 0.8508 - accuracy: 0.7084\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.9641 - accuracy: 0.6675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9641452431678772, 0.6675000190734863]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(32,32,3)), # 32x32  - 3 channels RGB\n",
    "        layers.Conv2D(32, (3,3), padding='valid', activation='relu'),  # padding='same' -> same shape;  padding='valid' -> shape changes;\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        layers.Conv2D(64,3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#model.summary()\n",
    "model.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits =True),\n",
    "    optimizer = keras.optimizers.Adam(3e-4),\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "print(X_train.shape, y_train.dtype)\n",
    "#model.summary()\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10)\n",
    "model.evaluate(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4262b307-1076-47ea-b69b-520a498967d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 78s 96ms/step - loss: 1.2875 - accuracy: 0.5459\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 81s 103ms/step - loss: 0.8924 - accuracy: 0.6887\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 85s 109ms/step - loss: 0.7271 - accuracy: 0.7479\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 88s 113ms/step - loss: 0.6134 - accuracy: 0.7870\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 84s 108ms/step - loss: 0.5155 - accuracy: 0.8226\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 85s 108ms/step - loss: 0.4355 - accuracy: 0.8510\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 90s 115ms/step - loss: 0.3613 - accuracy: 0.8774\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 87s 112ms/step - loss: 0.2932 - accuracy: 0.9021\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.2414 - accuracy: 0.9203\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 76s 97ms/step - loss: 0.1891 - accuracy: 0.9386\n",
      "157/157 [==============================] - 6s 33ms/step - loss: 1.0285 - accuracy: 0.7107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.028511881828308, 0.7106999754905701]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functional API\n",
    "def build_model():\n",
    "    inputs = keras.Input(shape=(32,32,3))\n",
    "    x = layers.Conv2D(32,3)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64,5,padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.Conv2D(128,3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(10)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = keras.optimizers.Adam(3e-4),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10)\n",
    "model.evaluate(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d756815-360f-48a5-8dda-0e010ac5b899",
   "metadata": {},
   "source": [
    "Here as we can see, the training accuracy is too good.. but accuracy on testing data is not that good. This means, the model is overfitting to the training data. So regularization methods should be employed. These regularizations should reduce the gap between training accuracy and the evaluation accuracy. <br>\n",
    "\n",
    "**Regularization Techniques - Reducing overfitting** <br>\n",
    "  - Reduce the capacity of model\n",
    "  - L2 Regularization\n",
    "  - Dropout\n",
    "  - Early Stoppage\n",
    "  - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43c2c306-1a27-42dd-9d3d-de2f546634b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "782/782 [==============================] - 100s 126ms/step - loss: 3.0553 - accuracy: 0.1252\n",
      "Epoch 2/150\n",
      "782/782 [==============================] - 109s 140ms/step - loss: 2.3029 - accuracy: 0.1411\n",
      "Epoch 3/150\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 2.2356 - accuracy: 0.1462\n",
      "Epoch 4/150\n",
      "782/782 [==============================] - 98s 126ms/step - loss: 2.2188 - accuracy: 0.1499\n",
      "Epoch 5/150\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 2.2086 - accuracy: 0.1506\n",
      "Epoch 6/150\n",
      "782/782 [==============================] - 97s 124ms/step - loss: 2.2016 - accuracy: 0.1499\n",
      "Epoch 7/150\n",
      "782/782 [==============================] - 94s 120ms/step - loss: 2.1980 - accuracy: 0.1530\n",
      "Epoch 8/150\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 2.1977 - accuracy: 0.1524\n",
      "Epoch 9/150\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 2.1895 - accuracy: 0.1552\n",
      "Epoch 10/150\n",
      "782/782 [==============================] - 134s 172ms/step - loss: 2.1878 - accuracy: 0.1548\n",
      "Epoch 11/150\n",
      "782/782 [==============================] - 194s 248ms/step - loss: 2.1845 - accuracy: 0.1561\n",
      "Epoch 12/150\n",
      "782/782 [==============================] - 185s 236ms/step - loss: 2.1868 - accuracy: 0.1586\n",
      "Epoch 13/150\n",
      "782/782 [==============================] - 166s 212ms/step - loss: 2.1808 - accuracy: 0.1561\n",
      "Epoch 14/150\n",
      "782/782 [==============================] - 173s 221ms/step - loss: 2.1829 - accuracy: 0.1580\n",
      "Epoch 15/150\n",
      "782/782 [==============================] - 181s 232ms/step - loss: 2.1780 - accuracy: 0.1569\n",
      "Epoch 16/150\n",
      "782/782 [==============================] - 176s 225ms/step - loss: 2.1766 - accuracy: 0.1605\n",
      "Epoch 17/150\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 2.1749 - accuracy: 0.1588\n",
      "Epoch 18/150\n",
      "782/782 [==============================] - 176s 225ms/step - loss: 2.1688 - accuracy: 0.1622\n",
      "Epoch 19/150\n",
      "782/782 [==============================] - 172s 220ms/step - loss: 2.1691 - accuracy: 0.1606\n",
      "Epoch 20/150\n",
      "782/782 [==============================] - 169s 217ms/step - loss: 2.1658 - accuracy: 0.1639\n",
      "Epoch 21/150\n",
      "782/782 [==============================] - 172s 219ms/step - loss: 2.1668 - accuracy: 0.1646\n",
      "Epoch 22/150\n",
      "782/782 [==============================] - 162s 207ms/step - loss: 2.1623 - accuracy: 0.1629\n",
      "Epoch 23/150\n",
      "782/782 [==============================] - 99s 127ms/step - loss: 2.1653 - accuracy: 0.1631\n",
      "Epoch 24/150\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 2.1596 - accuracy: 0.1633\n",
      "Epoch 25/150\n",
      "782/782 [==============================] - 100s 127ms/step - loss: 2.1650 - accuracy: 0.1639\n",
      "Epoch 26/150\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 2.1644 - accuracy: 0.1640\n",
      "Epoch 27/150\n",
      "782/782 [==============================] - 99s 126ms/step - loss: 2.1630 - accuracy: 0.1639\n",
      "Epoch 28/150\n",
      "782/782 [==============================] - 99s 127ms/step - loss: 2.1610 - accuracy: 0.1640\n",
      "Epoch 29/150\n",
      "782/782 [==============================] - 98s 126ms/step - loss: 2.1640 - accuracy: 0.1624\n",
      "Epoch 30/150\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 2.1607 - accuracy: 0.1644\n",
      "Epoch 31/150\n",
      "782/782 [==============================] - 97s 124ms/step - loss: 2.1608 - accuracy: 0.1625\n",
      "Epoch 32/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1647 - accuracy: 0.1651\n",
      "Epoch 33/150\n",
      "782/782 [==============================] - 97s 125ms/step - loss: 2.1560 - accuracy: 0.1660\n",
      "Epoch 34/150\n",
      "782/782 [==============================] - 98s 126ms/step - loss: 2.1609 - accuracy: 0.1663\n",
      "Epoch 35/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1619 - accuracy: 0.1630\n",
      "Epoch 36/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1574 - accuracy: 0.1642\n",
      "Epoch 37/150\n",
      "782/782 [==============================] - 100s 127ms/step - loss: 2.1618 - accuracy: 0.1663\n",
      "Epoch 38/150\n",
      "782/782 [==============================] - 100s 127ms/step - loss: 2.1592 - accuracy: 0.1635\n",
      "Epoch 39/150\n",
      "782/782 [==============================] - 97s 124ms/step - loss: 2.1606 - accuracy: 0.1649\n",
      "Epoch 40/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1617 - accuracy: 0.1630\n",
      "Epoch 41/150\n",
      "782/782 [==============================] - 97s 124ms/step - loss: 2.1609 - accuracy: 0.1635\n",
      "Epoch 42/150\n",
      "782/782 [==============================] - 97s 124ms/step - loss: 2.1491 - accuracy: 0.1673\n",
      "Epoch 43/150\n",
      "782/782 [==============================] - 97s 125ms/step - loss: 2.1536 - accuracy: 0.1660\n",
      "Epoch 44/150\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 2.1604 - accuracy: 0.1666\n",
      "Epoch 45/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1561 - accuracy: 0.1659\n",
      "Epoch 46/150\n",
      "782/782 [==============================] - 101s 130ms/step - loss: 2.1550 - accuracy: 0.1647\n",
      "Epoch 47/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1523 - accuracy: 0.1678\n",
      "Epoch 48/150\n",
      "782/782 [==============================] - 31342s 40s/step - loss: 2.1553 - accuracy: 0.1671\n",
      "Epoch 49/150\n",
      "782/782 [==============================] - 121s 154ms/step - loss: 2.1509 - accuracy: 0.1673\n",
      "Epoch 50/150\n",
      "782/782 [==============================] - 137s 175ms/step - loss: 2.1557 - accuracy: 0.1659\n",
      "Epoch 51/150\n",
      "782/782 [==============================] - 136s 175ms/step - loss: 2.1510 - accuracy: 0.1683\n",
      "Epoch 52/150\n",
      "782/782 [==============================] - 128s 163ms/step - loss: 2.1533 - accuracy: 0.1668\n",
      "Epoch 53/150\n",
      "782/782 [==============================] - 127s 162ms/step - loss: 2.1539 - accuracy: 0.1682\n",
      "Epoch 54/150\n",
      "782/782 [==============================] - 126s 161ms/step - loss: 2.1526 - accuracy: 0.1685\n",
      "Epoch 55/150\n",
      "782/782 [==============================] - 121s 155ms/step - loss: 2.1558 - accuracy: 0.1649\n",
      "Epoch 56/150\n",
      "782/782 [==============================] - 120s 153ms/step - loss: 2.1526 - accuracy: 0.1648\n",
      "Epoch 57/150\n",
      "782/782 [==============================] - 125s 160ms/step - loss: 2.1576 - accuracy: 0.1673\n",
      "Epoch 58/150\n",
      "782/782 [==============================] - 127s 162ms/step - loss: 2.1497 - accuracy: 0.1690\n",
      "Epoch 59/150\n",
      "782/782 [==============================] - 135s 172ms/step - loss: 2.1529 - accuracy: 0.1656\n",
      "Epoch 60/150\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 2.1536 - accuracy: 0.1684\n",
      "Epoch 61/150\n",
      "782/782 [==============================] - 119s 152ms/step - loss: 2.1533 - accuracy: 0.1685\n",
      "Epoch 62/150\n",
      "782/782 [==============================] - 127s 162ms/step - loss: 2.1496 - accuracy: 0.1688\n",
      "Epoch 63/150\n",
      "782/782 [==============================] - 124s 158ms/step - loss: 2.1499 - accuracy: 0.1686\n",
      "Epoch 64/150\n",
      "782/782 [==============================] - 121s 155ms/step - loss: 2.1510 - accuracy: 0.1688\n",
      "Epoch 65/150\n",
      "782/782 [==============================] - 124s 159ms/step - loss: 2.1513 - accuracy: 0.1683\n",
      "Epoch 66/150\n",
      "782/782 [==============================] - 122s 156ms/step - loss: 2.1511 - accuracy: 0.1674\n",
      "Epoch 67/150\n",
      "782/782 [==============================] - 118s 151ms/step - loss: 2.1481 - accuracy: 0.1692\n",
      "Epoch 68/150\n",
      "782/782 [==============================] - 124s 158ms/step - loss: 2.1539 - accuracy: 0.1682\n",
      "Epoch 69/150\n",
      "782/782 [==============================] - 123s 158ms/step - loss: 2.1476 - accuracy: 0.1701\n",
      "Epoch 70/150\n",
      "782/782 [==============================] - 117s 149ms/step - loss: 2.1507 - accuracy: 0.1676\n",
      "Epoch 71/150\n",
      "782/782 [==============================] - 123s 158ms/step - loss: 2.1480 - accuracy: 0.1694\n",
      "Epoch 72/150\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 2.1561 - accuracy: 0.1674\n",
      "Epoch 73/150\n",
      "782/782 [==============================] - 116s 149ms/step - loss: 2.1539 - accuracy: 0.1666\n",
      "Epoch 74/150\n",
      "782/782 [==============================] - 116s 149ms/step - loss: 2.1486 - accuracy: 0.1686\n",
      "Epoch 75/150\n",
      "782/782 [==============================] - 121s 155ms/step - loss: 2.1492 - accuracy: 0.1670\n",
      "Epoch 76/150\n",
      "782/782 [==============================] - 5785s 7s/step - loss: 2.1510 - accuracy: 0.1692\n",
      "Epoch 77/150\n",
      "782/782 [==============================] - 118s 151ms/step - loss: 2.1506 - accuracy: 0.1659\n",
      "Epoch 78/150\n",
      "782/782 [==============================] - 123s 157ms/step - loss: 2.1466 - accuracy: 0.1695\n",
      "Epoch 79/150\n",
      "782/782 [==============================] - 126s 161ms/step - loss: 2.1482 - accuracy: 0.1704\n",
      "Epoch 80/150\n",
      "782/782 [==============================] - 120s 153ms/step - loss: 2.1514 - accuracy: 0.1654\n",
      "Epoch 81/150\n",
      "782/782 [==============================] - 117s 150ms/step - loss: 2.1490 - accuracy: 0.1690\n",
      "Epoch 82/150\n",
      "782/782 [==============================] - 117s 150ms/step - loss: 2.1522 - accuracy: 0.1675\n",
      "Epoch 83/150\n",
      "782/782 [==============================] - 118s 150ms/step - loss: 2.1497 - accuracy: 0.1672\n",
      "Epoch 84/150\n",
      "782/782 [==============================] - 118s 151ms/step - loss: 2.1480 - accuracy: 0.1691\n",
      "Epoch 85/150\n",
      "782/782 [==============================] - 118s 151ms/step - loss: 2.1478 - accuracy: 0.1702\n",
      "Epoch 86/150\n",
      "782/782 [==============================] - 119s 152ms/step - loss: 2.1489 - accuracy: 0.1676\n",
      "Epoch 87/150\n",
      "782/782 [==============================] - 118s 151ms/step - loss: 2.1486 - accuracy: 0.1699\n",
      "Epoch 88/150\n",
      "782/782 [==============================] - 117s 150ms/step - loss: 2.1458 - accuracy: 0.1706\n",
      "Epoch 89/150\n",
      "782/782 [==============================] - 116s 149ms/step - loss: 2.1488 - accuracy: 0.1686\n",
      "Epoch 90/150\n",
      "782/782 [==============================] - 116s 148ms/step - loss: 2.1484 - accuracy: 0.1713\n",
      "Epoch 91/150\n",
      "782/782 [==============================] - 119s 153ms/step - loss: 2.1462 - accuracy: 0.1703\n",
      "Epoch 92/150\n",
      "782/782 [==============================] - 115s 147ms/step - loss: 2.1512 - accuracy: 0.1681\n",
      "Epoch 93/150\n",
      "782/782 [==============================] - 116s 149ms/step - loss: 2.1446 - accuracy: 0.1699\n",
      "Epoch 94/150\n",
      "782/782 [==============================] - 584s 748ms/step - loss: 2.1488 - accuracy: 0.1696\n",
      "Epoch 95/150\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 2.1455 - accuracy: 0.1704\n",
      "Epoch 96/150\n",
      "782/782 [==============================] - 14163s 18s/step - loss: 2.1468 - accuracy: 0.1695\n",
      "Epoch 97/150\n",
      "782/782 [==============================] - 93s 119ms/step - loss: 2.1459 - accuracy: 0.1694\n",
      "Epoch 98/150\n",
      "782/782 [==============================] - 91s 117ms/step - loss: 2.1472 - accuracy: 0.1726\n",
      "Epoch 99/150\n",
      "782/782 [==============================] - 91s 117ms/step - loss: 2.1500 - accuracy: 0.1680\n",
      "Epoch 100/150\n",
      "782/782 [==============================] - 90s 116ms/step - loss: 2.1498 - accuracy: 0.1697\n",
      "Epoch 101/150\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 2.1445 - accuracy: 0.1715\n",
      "Epoch 102/150\n",
      "782/782 [==============================] - 98s 126ms/step - loss: 2.1479 - accuracy: 0.1703\n",
      "Epoch 103/150\n",
      "782/782 [==============================] - 96s 122ms/step - loss: 2.1482 - accuracy: 0.1685\n",
      "Epoch 104/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1520 - accuracy: 0.1678\n",
      "Epoch 105/150\n",
      "782/782 [==============================] - 97s 124ms/step - loss: 2.1444 - accuracy: 0.1715\n",
      "Epoch 106/150\n",
      "782/782 [==============================] - 99s 127ms/step - loss: 2.1486 - accuracy: 0.1682\n",
      "Epoch 107/150\n",
      "782/782 [==============================] - 96s 123ms/step - loss: 2.1457 - accuracy: 0.1691\n",
      "Epoch 108/150\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 2.1550 - accuracy: 0.1662\n",
      "Epoch 109/150\n",
      "782/782 [==============================] - 112s 143ms/step - loss: 2.1475 - accuracy: 0.1681\n",
      "Epoch 110/150\n",
      "782/782 [==============================] - 108s 139ms/step - loss: 2.1445 - accuracy: 0.1702\n",
      "Epoch 111/150\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 2.1492 - accuracy: 0.1673\n",
      "Epoch 112/150\n",
      "782/782 [==============================] - 101s 129ms/step - loss: 2.1490 - accuracy: 0.1681\n",
      "Epoch 113/150\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 2.1467 - accuracy: 0.1704\n",
      "Epoch 114/150\n",
      "782/782 [==============================] - 99s 126ms/step - loss: 2.1476 - accuracy: 0.1698\n",
      "Epoch 115/150\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 2.1441 - accuracy: 0.1710\n",
      "Epoch 116/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1468 - accuracy: 0.1693\n",
      "Epoch 117/150\n",
      "782/782 [==============================] - 99s 127ms/step - loss: 2.1442 - accuracy: 0.1695\n",
      "Epoch 118/150\n",
      "782/782 [==============================] - 99s 126ms/step - loss: 2.1446 - accuracy: 0.1717\n",
      "Epoch 119/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1440 - accuracy: 0.1722\n",
      "Epoch 120/150\n",
      "782/782 [==============================] - 99s 126ms/step - loss: 2.1490 - accuracy: 0.1687\n",
      "Epoch 121/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1493 - accuracy: 0.1707\n",
      "Epoch 122/150\n",
      "782/782 [==============================] - 98s 126ms/step - loss: 2.1444 - accuracy: 0.1717\n",
      "Epoch 123/150\n",
      "782/782 [==============================] - 97s 125ms/step - loss: 2.1466 - accuracy: 0.1703\n",
      "Epoch 124/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1467 - accuracy: 0.1715\n",
      "Epoch 125/150\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 2.1429 - accuracy: 0.1713\n",
      "Epoch 126/150\n",
      "782/782 [==============================] - 100s 127ms/step - loss: 2.1477 - accuracy: 0.1692\n",
      "Epoch 127/150\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 2.1514 - accuracy: 0.1667\n",
      "Epoch 128/150\n",
      "782/782 [==============================] - 99s 126ms/step - loss: 2.1501 - accuracy: 0.1685\n",
      "Epoch 129/150\n",
      "782/782 [==============================] - 99s 126ms/step - loss: 2.1443 - accuracy: 0.1696\n",
      "Epoch 130/150\n",
      "782/782 [==============================] - 98s 126ms/step - loss: 2.1482 - accuracy: 0.1686\n",
      "Epoch 131/150\n",
      "782/782 [==============================] - 101s 129ms/step - loss: 2.1477 - accuracy: 0.1680\n",
      "Epoch 132/150\n",
      "782/782 [==============================] - 99s 127ms/step - loss: 2.1441 - accuracy: 0.1703\n",
      "Epoch 133/150\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 2.1435 - accuracy: 0.1693\n",
      "Epoch 134/150\n",
      "782/782 [==============================] - 99s 127ms/step - loss: 2.1471 - accuracy: 0.1679\n",
      "Epoch 135/150\n",
      "782/782 [==============================] - 100s 127ms/step - loss: 2.1423 - accuracy: 0.1724\n",
      "Epoch 136/150\n",
      "782/782 [==============================] - 99s 127ms/step - loss: 2.1466 - accuracy: 0.1700\n",
      "Epoch 137/150\n",
      "782/782 [==============================] - 100s 127ms/step - loss: 2.1488 - accuracy: 0.1701\n",
      "Epoch 138/150\n",
      "782/782 [==============================] - 101s 129ms/step - loss: 2.1483 - accuracy: 0.1687\n",
      "Epoch 139/150\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 2.1466 - accuracy: 0.1708\n",
      "Epoch 140/150\n",
      "782/782 [==============================] - 99s 126ms/step - loss: 2.1449 - accuracy: 0.1702\n",
      "Epoch 141/150\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 2.1442 - accuracy: 0.1726\n",
      "Epoch 142/150\n",
      "782/782 [==============================] - 100s 128ms/step - loss: 2.1466 - accuracy: 0.1710\n",
      "Epoch 143/150\n",
      "782/782 [==============================] - 95s 122ms/step - loss: 2.1429 - accuracy: 0.1710\n",
      "Epoch 144/150\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 2.1405 - accuracy: 0.1719\n",
      "Epoch 145/150\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 2.1446 - accuracy: 0.1721\n",
      "Epoch 146/150\n",
      "782/782 [==============================] - 93s 119ms/step - loss: 2.1464 - accuracy: 0.1696\n",
      "Epoch 147/150\n",
      "782/782 [==============================] - 101s 129ms/step - loss: 2.1437 - accuracy: 0.1709\n",
      "Epoch 148/150\n",
      "782/782 [==============================] - 92s 118ms/step - loss: 2.1426 - accuracy: 0.1731\n",
      "Epoch 149/150\n",
      "782/782 [==============================] - 92s 118ms/step - loss: 2.1459 - accuracy: 0.1697\n",
      "Epoch 150/150\n",
      "782/782 [==============================] - 92s 117ms/step - loss: 2.1445 - accuracy: 0.1710\n",
      "157/157 [==============================] - 7s 39ms/step - loss: 2.0634 - accuracy: 0.2179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0633604526519775, 0.21789999306201935]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_regularized_model():\n",
    "    inputs = keras.Input(shape=(32,32,3))\n",
    "    x = layers.Conv2D(32,3, padding='same', kernel_regularizer = regularizers.l2(0.01))(inputs)\n",
    "    x = layers.BatchNormalization()(x)    # BatchNorm -> Faster Traning  +  Regularization effect\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64,5,padding='same', kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.Conv2D(128,3, padding='same', kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(10)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Using regularization like DropOut leads to longer training time\n",
    "# Drops a lot of connections\n",
    "model = build_regularized_model()\n",
    "model.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = keras.optimizers.Adam(3e-4),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=150)\n",
    "model.evaluate(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db4d55-4e99-4fd7-961a-d21555378ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
